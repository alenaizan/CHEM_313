{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbe4329",
   "metadata": {},
   "source": [
    "# Molecular Dynamics and Monte Carlo\n",
    "In quantum chemical studies, we usually focus on computing the properties of a molecule using one or a few structures. Typically, a geometry optimization is carried out in order to determine the minimum energy structure, and then the properties of the molecule are computed at the minimum or minima. This procedure is acceptable for small or rigid molecules which do not have a lot of structural flexibility. However, for larger molecules or for a big collection of small molecules, there will be numerous low energy structures that are accessible under typical conditions (e.g. ambient temperature). To get a more appropriate description of such systems, we need to explore their various possible configurations and obtain some *average* picture. Monte Carlo (MC) and molecular dynamics (MD) methods enable the exploration of the various possible configurations of the system.\n",
    "\n",
    "## Overview of Monte Carlo and Molecular Dynamics Methods\n",
    "MC and MD methods are powerful techniques for sampling the possible configurations of the system. The state of the system is completely specified by the positions and momenta of all the particles. The *phase space* is the space in which all possible states of the system (positions and momenta) are represented. \n",
    "\n",
    "In MC methods, we start from some initial structure of the system. Then, we *randomly* perturb the coordinates of a randomly chosen particle. In the popular *Metropolis* approach, we always accept the move if the energy of the system decreases. If the energy increases, we generate a random number between 0 and 1 and accept the move if $e^{-\\Delta E/kT}$ is larger than the random number. This acceptance and rejection criteria allows overcoming potential energy barriers and the escape from local minima. Typically, the perturbation should be small so as to have a reasonable acceptance ration. However, this means that only a small portion of the configurational space can be explored. The Metropolis algorithm depends on the temperature and therefore MC methods are normally perform at constant number of particle, volume, and temperature (NVT) or the canonical ensemble. \n",
    "\n",
    "There are several advantages of MC methods. First, the perturbations can be performed in Cartersian or internal coordinates. Second, only energy evaluation is required and calculating derivatives of the energy is not needed. Third, because only one (or a few) particle is perturbed at each step, only the energy change associated with the perturbation needs to be calculated. Finally, it is easy to constrain certain degrees of freedom. One disadvantage of MC methods is its indepdence on time and velocities, and thus MC methods cannot explore time-depedent phenomena or properties that depend on the momentum.\n",
    "\n",
    "MD methods explore the evolution of the system over time by integrating Newton's second law ($\\textbf{F}=m \\textbf{a}$). A small time step (typically $\\sim 1$ fs) is usually used and therefore $10^6$ steps only explore the evolution of the system in 1 ns. Some physical processes, such as protein folding, occur at long time scales, and therefore short MD simulations only sample the region of the phase space close to the initial structure. Because the forces determine the *trajectory* of the particles, standard MD simulations cannot easily climb potential energy barriers because they will experience a force in the opposite direction. MD simulations are in principle deterministic, but very small numerical differences ($\\sim 10^{-8}$) can rapidly cause the irreproducibility of MD trajectories. Because MD simulations have velocity and time dependence, they can explore time-dependent properties like transport phenomena. Unlike MC methods, MD simulations require the calculation of both the energy and its derivative. Furthermore, all the particles move each step, unlike MC methods. MD simulations are performed in Cartesian coordinates. Imposing constraints on the degrees of freedom is more challenging than in MC methods. For MD methods, the natural ensemble is the microcanonical ensemble (NVE; constant number of particles, volume, and energy), though other ensembles can also be generated. \n",
    "\n",
    "The differences between MC and MD methods are summarized below:\n",
    "<img src=\"mc_vs_md.png\" alt=\"drawing\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af318d7a",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "The basic idea of Monte Carlo methods is the random sampling of the phase space of molecules. As summarized above, a perurbation to the molecular structure is introduced either in Cartesian or internal coordinates. The move is always accepted if it lowers the energy. If it raises the energy, the move is accepted if $e^{-\\Delta E/kT}$ is larger than a randomly generated number between 0 and 1.\n",
    "\n",
    "### Random Number Generation\n",
    "Typically, numbers are generated by a pseduo-random number generator, where the numbers are not truly random. The key point for random number generators is that they generate a very long sequence of apparently uncorrelated numbers. However, after a long sequence, the numbers will be repeated exactly. Usually, the sequence of (random) numbers can be fixed by using a given initial value (called *seed*). However, depending on the implementation, it may not be possible to generate the same sequence of numbers if different compilers or different computer architectures are used. Using an initial seed can be useful for debugging the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1882f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numpy has different functionalities for random number generation\n",
    "import numpy as np\n",
    "\n",
    "# Generate an array of 6 random numbers\n",
    "print(\"Random numbers 1:\", np.random.rand(6))\n",
    "\n",
    "# Generate a second array of 6 random numbers; They will be different\n",
    "print(\"Random numbers 2:\", np.random.rand(6))\n",
    "\n",
    "# Now fix the seed using an integer value of zero\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate an array of 6 random numbers\n",
    "print(\"Random numbers 1 (fixed seed):\", np.random.rand(6))\n",
    "\n",
    "# Now again set the seed value\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate a second array of 6 random numbers; The numbers will be the same\n",
    "print(\"Random numbers 2 (same seed):\", np.random.rand(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd3a53",
   "metadata": {},
   "source": [
    "### Size of Perturbation Step\n",
    "The size of the perturbation is an important parameter. A small size will lead to higher acceptance ratio, but it will lead to a slow exploration of the phase space. A large step will lead to a smaller acceptance ratio, and a sampling that consists of wide jumps.\n",
    "\n",
    "### Nonphysical Moves\n",
    "Because the perturbations are random, they can make physically unrealistic moves. This can be an advantage because this allows the Monte Carlo procedure to tunnel through high energy barrier. However, sometimes this can be a problem. For example, the chirality of the molecule may be inverted in a random move but this might be an undesired change.\n",
    "\n",
    "### Correlated Motions\n",
    "Because only one (or a few) particles are moved in each step, it is difficult to explore correlated changes in the configuration. For example, the dihedral angles in the protein backbone are correlated, and only certain combinations of values give an acceptable structure. However, a small perturbation in only one dihedral angle may raise the energy substantially because of atom clashes, and thus such a move will likely be rejected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11640ad6",
   "metadata": {},
   "source": [
    "## Molecular Dynamics\n",
    "In molecular dynamics simulations, we use Newton's second law $\\textbf{F} = m \\textbf{a}$ to describe the evolution of the system over time. In the differential form, Newton's second law is given by:\n",
    "\\begin{align}\n",
    "    -\\frac{dV}{d\\textbf{r}} = m \\frac{d^2 \\textbf{r}}{dt^2}\n",
    "\\end{align}\n",
    "where $V$ is the potential energy at position $\\textbf{r}$. Given an initial configuration $\\textbf{r}_i$, we can determine the configuration at any other time by integrating Newton's equation. Except for a few simple cases, the integration cannot be performed analytically and therfore numerical integration is needed.\n",
    "\n",
    "### Numerical Integrators\n",
    "#### The Verlet Algorithm\n",
    "The Verlet algorithm is a common approach for integrating Newton's equation. Given an initial configuration $\\textbf{r}_i$, the position after a small time step $\\Delta t$ is given by the Taylor expansion:\n",
    "\\begin{align}\n",
    "    \\textbf{r}_{i+1} &= \\textbf{r}_i + \\frac{\\partial \\textbf{r}}{\\partial t} (\\Delta t) + \n",
    "                              + \\frac{1}{2} \\frac{\\partial^2 \\textbf{r}}{\\partial t^2} (\\Delta t)^2 +\n",
    "                              + \\frac{1}{6} \\frac{\\partial^3 \\textbf{r}}{\\partial t^3} (\\Delta t)^3 + \\cdots\\\\\n",
    "    \\textbf{r}_{i+1} &= \\textbf{r}_i + \\textbf{v}_i (\\Delta t) +  \\frac{1}{2} \\textbf{a}_i (\\Delta t)^2 +\n",
    "                        \\frac{1}{6} \\textbf{b}_i (\\Delta t)^3 + \\cdots\n",
    "\\end{align}\n",
    "where $\\textbf{v}_i, \\textbf{a}_i, \\textbf{b}_i$ are the velocity, acceleration, and hyper-acceleration at time $t_i$.\n",
    "\n",
    "Similarly, the position at time step $\\Delta t$ earlier is given by substituting $\\Delta t$ with $-\\Delta t$:\n",
    "\\begin{align}\n",
    "    \\textbf{r}_{i-1} &= \\textbf{r}_i - \\textbf{v}_i (\\Delta t) +  \\frac{1}{2} \\textbf{a}_i (\\Delta t)^2 -\n",
    "                        \\frac{1}{6} \\textbf{b}_i (\\Delta t)^3 + \\cdots  \n",
    "\\end{align}\n",
    "\n",
    "Addition of the two above equations gives a formula for calculating the position at time $\\Delta t$ given the two previous positions and the current acceleration:\n",
    "\\begin{align}\n",
    "    \\textbf{r}_{i+1} &= (2 \\textbf{r}_i - \\textbf{r}_{i-1}) + \\textbf{a}_i (\\Delta t)^2 + \\cdots \\\\\n",
    "    \\textbf{a}_i &= \\frac{\\textbf{F}_i}{m_i} = -\\frac{1}{m_i} \\frac{dV}{d \\textbf{r}_i}.\n",
    "\\end{align}\n",
    "\n",
    "This is the Verlet algorithm. The term involving $\\textbf{b}$ disappears and therefore this expression is correct to third order in $\\Delta t$. At the initial point, the previous position is estimated from the initial position and the initial velocity:\n",
    "\\begin{align}\n",
    "    \\textbf{r}_{-1} = \\textbf{r}_0 - \\textbf{v}_0 \\Delta t\n",
    "\\end{align}\n",
    "\n",
    "As the time step $\\Delta t$ decreases, the approximation increases in accuracy. However, a small time step requires many simulation steps to reach a given total simulation time.\n",
    "\n",
    "The velocities do not appear in the equations, which complicates calculation and the control of the temperature. The velocity can be estimated from the previous and next positions:\n",
    "\\begin{align}\n",
    "    \\textbf{v}_i = \\frac{\\textbf{r}_{i+1} - \\textbf{r}_{i-1}}{2 \\Delta t}.\n",
    "\\end{align}\n",
    "This, however, has the numerical problem that the numerator involves the subtraction of two similar numbers divided by a small number. This can lead to numerical inaccuracies.\n",
    "\n",
    "#### The Leap-frog Algorithm\n",
    "The leap-frog algorithm solves some of the numerical problems of the Verlet algorithm and utilizes the velocities. Performing a similar expansion at a half time step later and subtracting gives:\n",
    "\\begin{align}\n",
    "    \\textbf{r}_{i + 1} = \\textbf{r}_i + \\textbf{v}_{i + \\frac{1}{2}} \\Delta t.\n",
    "\\end{align}\n",
    "The velocity can be estimated using a similar expansion to get:\n",
    "\\begin{align}\n",
    "    \\textbf{v}_{i + \\frac{1}{2}} = \\textbf{v}_{i - \\frac{1}{2}} + \\textbf{a}_i \\Delta t\n",
    "\\end{align}\n",
    "\n",
    "Again, the expressions are correct to third order but it has better numerical accuracy than the Verlet algorithm. The disadvantage is that the position and velocity are not known at the same time. However, the explicit presence of the velocities allow for controling the temperature.\n",
    "\n",
    "#### Velocity Verlet\n",
    "In the velocity Verlet algorithm, both the velocity and the position are determined at the same time. The equations are given by:\n",
    "\\begin{align}\n",
    "    \\textbf{r}_{i+1} &= \\textbf{r}_i + \\textbf{v}_i \\Delta t + \\frac{1}{2} \\textbf{a}_i (\\Delta t)^2 \\\\\n",
    "    \\textbf{v}_{i+1} &= \\textbf{v}_i + \\frac{1}{2} ( \\textbf{a}_i + \\textbf{a}_{i+1}) \\Delta t\n",
    "\\end{align}\n",
    "\n",
    "#### Integration Time Step\n",
    "The fastest process in the system determines the maximum time step that can be taken, with the appropriate time step typically being an order of magnitude slower. Molecular rotations and vibrations occur with frequencies in the range of $10^{11}-10^{14}$ s$^{-1}$. Thus, typically a 1 fs ($10^{-15}$) time step is usually chosen. A million time step therefore simulates the system for only 1 ns, which may not be enough to explore some chemical systems.\n",
    "\n",
    "The fastest process is typically bond stretching that involves hydrogen atoms. These degrees of freedom, however, often have little influence on some chemical properties. Thus, bonds involving hydrogen atoms are often frozen using the *SHAKE* (Verlet), *LINCS* (leap-frog), or *RATTLE* (velocity verlet) algorithms. This allows a time step of 2 fs to be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30532c90",
   "metadata": {},
   "source": [
    "### Non-natural Ensembles\n",
    "The natural ensemble for solving Newton's equation is the microcanonical ensemble (NVE). In this ensemble, the pressure and temperature fluctuate. The total energy is the sum of the kinetic and potential energies:\n",
    "\\begin{align}\n",
    "    E_{\\rm tot} = \\sum_{i=1}^N \\frac{1}{2} m_i \\textbf{v}_i^2 + V(\\textbf{r})\n",
    "\\end{align}\n",
    "\n",
    "The average kinetic energy for a large number of particle $N$ is related to the temperature by\n",
    "\\begin{align}\n",
    "    \\langle E_{\\rm kin} \\rangle = \\frac{3}{2} N k_{\\rm B} T\n",
    "\\end{align}\n",
    "\n",
    "The microcanonical ensemble is not convenient because it is difficult to devise experimental conditions where the NVE variables are constant. Experimentally, it is easier to fix the temperature and pressure. Ensembles with fixed termperature/pressure can also be generated for MD simulations\n",
    "\n",
    "#### The Canonical Ensemble (NVT)\n",
    "In the canonical ensemble, the temperature is fixed by coupling the system to a large heat reservior. Because the temperature is related to the kinetic energy and the velocity of the particles, the temperature of the simulation can be controlled by scaling the velocity of all particles. Scaling the velocity at each time step introduces unwanted changes in the dynamics. Instead, usually a *thermostat* or a heat bath is used to add or remove energy from the system at suitable time intervals. A commonly used thermostat is the Nose-Hoover thermostat.\n",
    "\n",
    "#### The Isothermal-Isobaric Ensemble (NPT)\n",
    "In addition to controlling the temperature, the pressure can also be controlled to produce the isothermal-isobaric ensemble (NPT). Again, a *barostat* or a pressure bath can be used to control the pressure. Barostats work by scaling the volume of the system.\n",
    "\n",
    "#### Langevin and Brownian Dynamics\n",
    "Sometimes, the interest is in the dynmaics of a small system in the presence of a large number of other particles. The forces in the system can be decomposed to three terms:friction\n",
    "- A friction force proprtional to the velocity with a friction coefficient $\\xi$ (Removes energy from the system).\n",
    "- The intramolecular forces of the system\n",
    "- A random force associated with the temperature and averages to zero (Add energy to the system).\n",
    "With these forces, Newton's equation takes the form\n",
    "\\begin{align}\n",
    "    m \\frac{d^2 \\textbf{r}}{dt^2} = -\\xi \\frac{d \\textbf{r}}{dt} + \\textbf{F}_{\\rm intra} + \\textbf{F}_{\\rm random}\n",
    "\\end{align}\n",
    "This is called the *Langevin* equation of motion and it is a stochastic equation. The limit that the friction force is dominant ($\\xi$ is large) gives rise to *Brownian dynamics*. The Langevin integrator is often used to control the temperature in the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbaf1cc",
   "metadata": {},
   "source": [
    "### Periodic Boundary Conditions\n",
    "Molecular dynamics simulation are performed for relatively small systems. If the particles are not constrained somehow, some molecules may potentially boil off into space. Furthermore, for small systems, surface effects can become important. To address these limitations, *periodic boundary conditions* are usually used.\n",
    "\n",
    "When periodic boundary conditions are employed in the simulation, a box of the system become surrounded by other fictitious simulation boxes as illustrated in the image below:\n",
    "<img src=\"periodic_boundary_conditions.png\" alt=\"drawing\" width=\"200px\"/>\n",
    "\n",
    "If a particle escape from one side, then it re-appears in the opposite side because the system becomes periodic. A cubic box is often used but other types of boxes are also possible. Because electrostatic interactions is a long-range force in the system, it needs special treatment for periodic systems. Electrostatic interactions for periodic systems are usually calculated using the Ewald sum methods discussed before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00026b9",
   "metadata": {},
   "source": [
    "### Extracting Information from Simulations\n",
    "The main output of an MD simulation is the MD trajectory, which is the sequence of positions (and possibly momenta) of the particles at each time step. Furthermore, other properties, such as thermodynamic quantities, can be extracted from the simulations. We generally calculated *average* quantities from MD trajectories. A single MD snapshot does not represent a statistically meaningful observation.\n",
    "\n",
    "#### The Ergodic Hypothesis\n",
    "In statistical mechanics, we are concerned with *average* properties. A single particle (or a few particles) explores the phase space over time, and thus we can define an *average over time* as:\n",
    "\\begin{align}\n",
    "    \\langle X \\rangle = \\lim_{\\tau \\to \\infty} \\frac{1}{\\tau} \\int_0^\\tau X(t) dt\n",
    "\\end{align}\n",
    "Alternatively, for a large number of identical particles, each particle will occupy a certain region of the phase space at any given time. Thus, we can also calculate the *ensemble average* as:\n",
    "\\begin{align}\n",
    "    \\langle X \\rangle = \\lim_{M \\to \\infty} \\frac{1}{M} \\sum_{i=1}^M X_i\n",
    "\\end{align}\n",
    "\n",
    "The *ergodic hypothesis* asserts that these two averages are equal.\n",
    "\n",
    "#### Equilibration and Production Stages\n",
    "To perform an MD simulation, an initial structure is required. This is often an X-ray structure of the system if available or some guess structure generated by a modeling program. This initial structure is usually high in energy. To prevent the presence of strong forces in the beginning which can destabilize the simulation, energy minimization to a nearby local minima is performed before running the simulation. Still, the structure in the beginning of the simulation can still be far from equilibrium.\n",
    "\n",
    "The above equations for the averages assume that the system is at equilibrium. Therefore, an MD trajectory is usually divided to two stages: *equilibration* and *production*. The equilibration stage is supposed to bring the system to the equilibrium structure. Data from the equilibration stage are not used to calculate the averages. Only data from the production stage are used.\n",
    "\n",
    "There is no strict way to partition the trajectory to equilibration and production stages. Often, some initial portion of the trajectory is discarded as equilibration. Discarding too little can bias the results towards the initial structure while discarding too much can increase the statistical error in the average data. Generally, thermodynamic properties, such as the energy and the temperature, can be monitored to establish whether they reached stable values with limited fluctuations.\n",
    "\n",
    "#### Uncorrelated Samples\n",
    "The above averages assume that the samples are *uncorrelated*. In MD simulations, snapshots should be saved at suitable intervals of time where the property of interest is uncorrelated. Saving too many snapshots can take a lot of storage space while adding little statistical significance. Different ways can be used to estimate the correlation times for different properties.\n",
    "\n",
    "#### Some Properties\n",
    "\n",
    "##### Temperature\n",
    "The temperature can be calculated from the average kinetic energy as indicated above\n",
    "\n",
    "##### Internal Energy\n",
    "The internal energy is just calculated from the force field energy at each time step. It will typically fluctuate around an average value.\n",
    "\n",
    "##### Heat Capacity\n",
    "The heat capacity at constant volume is the derivative of the energy with respect to temperature at constant volume:\n",
    "\\begin{align}\n",
    "    C_V = \\left ( \\frac{\\partial U}{\\partial T} \\right )_V\n",
    "\\end{align}\n",
    "It can be estimated by running the simulation at different temperatures, estimating $\\langle U \\rangle$ as a function of temperature, and taking its derivative. However, this is inconvenient because it requires multiple simulations.\n",
    "\n",
    "The heat capacity can also be obtained from the fluctuation in the internal energy as\n",
    "\\begin{align}\n",
    "    C_V = \\frac{1}{kT^2} ( \\langle U^2 \\rangle - \\langle U \\rangle^2)\n",
    "\\end{align}\n",
    "The correlation time for the fluctuations is generally larger than the correlation time for the internal energy, and thus longer simulations may be required to converge the heat capacity\n",
    "\n",
    "##### Radial Distribution Function\n",
    "The radial distribution function $g(r)$ measures the relative probability of finding a reference particle as a function of the distance from other particles. The probability is relative to a completely uniform distribution, which has a density of $N/V$. The figure below illustrates the calculation of $g(r)$:\n",
    "<img src=\"rdf.png\" alt=\"drawing\" width=\"200px\"/>\n",
    "\n",
    "A typical behavior of the radial distribution function is shown below:\n",
    "<img src=\"rdf_plot.png\" alt=\"drawing\" width=\"500px\"/>\n",
    "\n",
    "$g(r)$ starts at zero because the size of the reference particle excludes other particles. The first peak correseponds to the first solvation shell, and the second peak corresponds to the second solvation shell, etc. $g(r)$ levels off to 1 because this is the relative probability for a uniform distribution of particles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758aa526",
   "metadata": {},
   "source": [
    "### Free Energy Methods\n",
    "Various methods can be used to estimate the free energy difference between two states, e.g. Helmholtz or Gibbs free energy difference. A key point about the free energy difference is that it is a *state function*. That is, it does not depend on the path that connects the two states. This allows molecular simulation to use any path, even physically impossible paths, to estimate the difference between the two states.\n",
    "\n",
    "#### Free Energy Perturbation\n",
    "The free energy difference between states A and B can be written as an ensemble average\n",
    "\\begin{align}\n",
    "    \\langle \\Delta A_{\\rm AB} \\rangle = k T {\\rm ln} \\langle e^{(E_{\\rm B} - E_{\\rm A})/kT}\\rangle\n",
    "\\end{align}\n",
    "In practice, to converge the free energy difference, the states A and B must be sufficiently similar. To facilitate this, the energy of the system in terms of a parameter $\\lambda$ that connects the two states A and B:\n",
    "\\begin{align}\n",
    "    E_\\lambda = \\lambda E_{\\rm A} + (1-\\lambda) E_{\\rm B}.\n",
    "\\end{align}\n",
    "where $\\lambda$ is from zero to one. Then, the energy difference is calculated for different values of $\\lambda$.\n",
    "\n",
    "To validate the calculated free energies, the transformation is usually carried out in both directions (A $\\rightarrow$ B and B $\\rightarrow$ A). The difference in energy between the two transformations gives an estimate of the convergence of the free energy.\n",
    "\n",
    "#### Thermodynamic Integration\n",
    "In thermodynamic integration, the energy of the system is again written in terms of the parameter $\\lambda$. The free energy difference is then estimated as\n",
    "\\begin{align}\n",
    "    \\Delta A = \\sum_i \\left \\langle \\frac{\\partial E(\\lambda)}{\\partial \\lambda} \\right \\rangle d \\lambda_i\n",
    "\\end{align}\n",
    "where again different values of $\\lambda$ are used.\n",
    "\n",
    "#### Umbrella Sampling\n",
    "In umbrella sampling, a bias is introduced in order to allow the system to overcome potential energy barrier along some reaction coordinate or degree of freedom of the system. Then, the unbiased distribution is estimated from the biased simulations and the free energy is calculated. The free energy along the chosen coordinate is called the *potential of mean force*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e84627",
   "metadata": {},
   "source": [
    "## Useful Resources\n",
    "\n",
    "- Cramer, C. J. *Essentials of Computational Chemistry: Theories and Models*, 2nd ed.; John Wiley & Sons: Chichester, England, 2004. (Chapters 3, 12, and 13)\n",
    "- Jensen, F. *Introduction to Computational Chemistry*, 3rd ed.; John Wiley & Sons: Nashville, TN, 2017. (Chapter 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b57095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
